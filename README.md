# ğŸ›° Radargram Segmentation â€” Supervised Framework
git config --global user.name "Milkisa T. Yebasse"
git config --global user.email "milkisa.yebasse@gmail.com"

### ğŸ“˜ Overview
This project provides a **PyTorch-based supervised training framework** for **semantic segmentation of radar sounder (RS) data**, such as MCoRDS or SHARAD radargrams.  
It enables training and evaluation across multiple architectures under a unified interface.

Supported model  literature architectures:
- ğŸ§© **UNet** â€“ classic encoder-decoder segmentation model  
- ğŸŒŠ **UNet-ASPP** â€“ UNet enhanced with Atrous Spatial Pyramid Pooling  
- âš¡ **Efficient UÂ²-Net** â€“ multi-task nested U-structure for efficient feature learning  
- ğŸ›° **TransSounder** â€“ transformer-based model for radar sounder data  

The script performs **fold-wise training**, manages checkpoints automatically, and allows for flexible configuration via argument presets.

---

### ğŸ§© Folder Structure
```
project_root/
â”‚
â”œâ”€â”€ implmentation/
â”‚   â”œâ”€â”€ dataset.py                  # mc10_data_model(), sharad_data_model(), etc.
â”‚   â”œâ”€â”€ inputs.py                   # Presets, argument parser, build_model(), loss functions
â”‚   â”œâ”€â”€ metrics.py                  # 
|   â”œâ”€â”€ output.py                   #
|
â”œâ”€â”€ literature/
â”‚   â”œâ”€â”€ unet.py                     # Standard UNet architecture
â”‚   â”œâ”€â”€ aspp.py                     # UNetASPP with Atrous Spatial Pyramid Pooling
â”‚   â”œâ”€â”€ transounder.py              # Transformer-based TransSounder model
â”‚   â”œâ”€â”€ u2net.py                    # Efficient UÂ²-Net model
â”‚
â”œâ”€â”€ data_loader.py                  # SalObjDataset, ToTensorLab utilities
â”œâ”€â”€ supervised_foldtrain.py         # Main training script (this file)
â””â”€â”€ README.md                       # Documentation (this file)
```

---

### âš™ï¸ Main Functionalities
âœ… Loads radargram data and cross-validation folds from `mc10_data_model()` or `sharad_data_model()`  
âœ… Dynamically builds the model via `parse_args()`, `apply_presets()`, and `build_model()`  
âœ… Supports multi-output and single-output architectures  
âœ… Performs iterative training with automatic model checkpointing  
âœ… Compatible with CUDA acceleration  
âœ… Modular design â€“ easily switch datasets or model types  

---

### ğŸ§  Data Preparation
Data loading example:
```python
rs_image, rs_label, folds, _ = mc10_data_model()
```

- `rs_image`: NumPy array of radargram images  
- `rs_label`: NumPy array of label masks  
- `folds`: list of (train_index, test_index) pairs from K-Fold cross-validation  

Each fold is processed as:
```python
for fold, (train_index, test_index) in enumerate(folds):
    train_images, test_images = rs_image[train_index], rs_image[test_index]
    train_labels, test_labels = rs_label[train_index], rs_label[test_index]
```

---

### ğŸ§± Model Setup
The framework dynamically builds and configures the model:
```python
args = parse_args()
model_kwargs, criterion = apply_presets(args)
net = build_model(args, model_kwargs)
```

Supported models via `args.model`:
| Argument | Model | File |
|-----------|--------|------|
| `unet` | UNet | `literature/unet.py` |
| `aspp` | UNet-ASPP | `literature/aspp.py` |
| `eu` | Efficient UÂ²-Net | `literature/u2net.py` |
| `transsounder` | TransSounder | `literature/transounder.py` |

Example:
```bash
python train_crossval.py --model eu --lr 1e-4 --weight_decay 5e-4
```

---

### ğŸ‹ï¸ Training Details
- **Epochs:** 5000  
- **Batch size:** 1  
- **Optimizer:** Adam (`lr`, `weight_decay` from presets)  
- **Loss functions:**
  - `muti_bce_loss_fusion()` for Efficient UÂ²-Net (multi-output)
  - `criterion` (e.g., `CrossEntropyLoss`) for other models  

Checkpoint saving frequency:
```python
save_frq = 20000  # saves every 20k iterations
```

---

### ğŸ’¾ Checkpoints
Checkpoints are automatically stored in the model directory defined in the preset:
```
model_dir/
â”‚
â””â”€â”€ greenland_<model>_fold<k>_iter<ite>_epoch<ep>_loss<val>_time<sec>_<timestamp>.pth
```

Example:
```
greenland_eu_fold1_iter20000_epoch10_loss0.125400_time134.2_20251016-143820.pth
```

---

### ğŸ“Š Example Console Output
```
Fold 1
Train Images shape: (200, 512, 512)
Train Labels shape: (200, 512, 512)
[OK] Model created: eu with 47,832,409 params
[OK] Optimizer created with: {'lr': 0.0001, 'weight_decay': 0.0005}
---start training...
[epoch: 005/5000, ite: 20000] train loss: 0.134521, tar: 0.000000
```

---

### ğŸ§© Customization
| Parameter | Description | Default |
|------------|--------------|----------|
| `epoch_num` | Number of training epochs | 5000 |
| `batch_size_train` | Training batch size | 1 |
| `save_frq` | Model saving frequency | 20000 |
| `args.model` | Model type | `"eu"` |
| `dataset` | Choose between `mc10_data_model()` or `sharad_data_model()` | `mc10` |

---

### ğŸ§® Requirements
Install dependencies:
```bash
pip install torch torchvision numpy matplotlib scikit-learn scipy
```

or using conda:
```bash
conda install pytorch torchvision numpy matplotlib scikit-learn scipy -c pytorch
```

---

### âš¡ GPU Usage
The script automatically uses CUDA if available:
```python
if torch.cuda.is_available():
    net.cuda()
```

To force CPU only:
```bash
CUDA_VISIBLE_DEVICES="" python train_crossval.py
```

---


### ğŸ‘¤ Author
**Milkisa T. Yebasse**  
Ph.D. Researcher â€” RS Lab, University of Trento  
ğŸ“§ milkisa.yebasse@unitn.it  
ğŸŒ [GitHub](https://github.com/milkisayebasse)
# supervised_radar-sounder

